{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7045013,"sourceType":"datasetVersion","datasetId":4053823}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORT LIBRARIES AND CHECK GPU","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install transformers datasets accelerate peft bitsandbytes -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:42:26.662111Z","iopub.execute_input":"2025-10-06T02:42:26.662353Z","iopub.status.idle":"2025-10-06T02:43:57.128889Z","shell.execute_reply.started":"2025-10-06T02:42:26.662334Z","shell.execute_reply":"2025-10-06T02:43:57.128054Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:44:10.338304Z","iopub.execute_input":"2025-10-06T02:44:10.338605Z","iopub.status.idle":"2025-10-06T02:44:49.765038Z","shell.execute_reply.started":"2025-10-06T02:44:10.338574Z","shell.execute_reply":"2025-10-06T02:44:49.764251Z"}},"outputs":[{"name":"stderr","text":"2025-10-06 02:44:29.857243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759718670.221344      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759718670.326356      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Check GPU\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}, GPUs: {torch.cuda.device_count()}\")\nif device == \"cuda\":\n    for i in range(torch.cuda.device_count()):\n        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:44:49.766260Z","iopub.execute_input":"2025-10-06T02:44:49.767107Z","iopub.status.idle":"2025-10-06T02:44:49.773391Z","shell.execute_reply.started":"2025-10-06T02:44:49.767086Z","shell.execute_reply":"2025-10-06T02:44:49.772394Z"}},"outputs":[{"name":"stdout","text":"Device: cuda, GPUs: 2\nGPU 0: Tesla T4\nGPU 1: Tesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# LOAD DATASET AND TOKENIZER","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/tinystories-narrative-classification/train.csv')\nval_df = pd.read_csv('/kaggle/input/tinystories-narrative-classification/validation.csv')\ntrain_df = train_df.sample(n=300_000, random_state=42)  # 2.1M -> 300K\nval_df = val_df.sample(n=3_000, random_state=42)  # 22K -> 3K\n\nprint(f\"Train: {len(train_df)}, Val: {len(val_df)}\")\n\ntrain_dataset = Dataset.from_pandas(train_df[['text']]).filter(lambda x: x[\"text\"] and str(x[\"text\"]).strip() != \"\")\nval_dataset   = Dataset.from_pandas(val_df[['text']]).filter(lambda x: x[\"text\"] and str(x[\"text\"]).strip() != \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T02:44:49.775074Z","iopub.execute_input":"2025-10-06T02:44:49.775440Z","iopub.status.idle":"2025-10-06T02:45:43.813381Z","shell.execute_reply.started":"2025-10-06T02:44:49.775415Z","shell.execute_reply":"2025-10-06T02:45:43.812598Z"}},"outputs":[{"name":"stdout","text":"Train: 300000, Val: 3000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/300000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed0c896bac849e085ceb8bf642a0dc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0585813dd25d43dd838e61f569af2efa"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"model_name = \"gpt2\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\n# Fixed length packing: concat -> split into blocks of 256\nBLOCK_SIZE = 256\n\ndef tokenize(examples):\n    return tokenizer(examples[\"text\"], add_special_tokens=False)\n\ndef pack_dataset(ds):\n    tokenized = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n    \n    def group_texts(examples):\n        concatenated = []\n        for ids in examples[\"input_ids\"]:\n            concatenated.extend(ids)\n        \n        total_len = (len(concatenated) // BLOCK_SIZE) * BLOCK_SIZE\n        \n        input_ids = [concatenated[i : i + BLOCK_SIZE] for i in range(0, total_len, BLOCK_SIZE)]\n        attention_mask = [[1]*BLOCK_SIZE for _ in range(len(input_ids))]\n        labels = [ids[:] for ids in input_ids]  # Deep copy\n        \n        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n    \n    # batched=False done or add remove_columns\n    packed = tokenized.map(\n        group_texts, \n        batched=True, \n        remove_columns=tokenized.column_names  # Eski kolonları kaldır\n    )\n    return packed\n\ntokenized_train = pack_dataset(train_dataset)\ntokenized_val   = pack_dataset(val_dataset)\n\nprint(f\"Train blocks: {len(tokenized_train)}, Val blocks: {len(tokenized_val)}\")\nprint(f\"Columns: {tokenized_train.column_names}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:03:36.599548Z","iopub.execute_input":"2025-10-06T03:03:36.600333Z","iopub.status.idle":"2025-10-06T03:08:24.496199Z","shell.execute_reply.started":"2025-10-06T03:03:36.600306Z","shell.execute_reply":"2025-10-06T03:08:24.495335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/299970 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"164ba31404194d3ead140b0fbe5fdbe1"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/299970 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd73b0e732246e891917c5d2b2d120f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e7fa19d71bf4045a140f361c78a7825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099aa11a95a442b4b08af42e1f8adfdf"}},"metadata":{}},{"name":"stdout","text":"Train blocks: 261045, Val blocks: 2538\nColumns: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# QLoRA - 4-BIT QUANTIZATION CONFIG","metadata":{}},{"cell_type":"code","source":"# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Model loading\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map={\"\": 0},\n    torch_dtype=torch.float16,\n)\n\n# Prepare model for QLoRA\nmodel = prepare_model_for_kbit_training(model)\nmodel.gradient_checkpointing_enable() # for memory saving","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:16:22.831135Z","iopub.execute_input":"2025-10-06T03:16:22.831438Z","iopub.status.idle":"2025-10-06T03:16:23.788686Z","shell.execute_reply.started":"2025-10-06T03:16:22.831420Z","shell.execute_reply":"2025-10-06T03:16:23.788140Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# LoRA CONFIGURATION","metadata":{}},{"cell_type":"code","source":"# LoRA config\nlora_config = LoraConfig(\n    r=4,  # Rank\n    lora_alpha=8,\n    target_modules=[\"c_attn\"],  # GPT-2 attention modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Add LoRA adapter to model\nmodel = get_peft_model(model, lora_config)\n\n# Trainable parameters\nmodel.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:16:34.471361Z","iopub.execute_input":"2025-10-06T03:16:34.471724Z","iopub.status.idle":"2025-10-06T03:16:34.496008Z","shell.execute_reply.started":"2025-10-06T03:16:34.471698Z","shell.execute_reply":"2025-10-06T03:16:34.495227Z"}},"outputs":[{"name":"stdout","text":"trainable params: 147,456 || all params: 124,587,264 || trainable%: 0.1184\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# TRAINING ARGUMENTS","metadata":{}},{"cell_type":"code","source":"output_dir = \"./qlora_story_model\"\n\nper_device_bs=8\ngrad_accum=4\nmax_steps=10_000\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    overwrite_output_dir=True,\n\n    per_device_train_batch_size=per_device_bs,\n    per_device_eval_batch_size=per_device_bs,\n    gradient_accumulation_steps=grad_accum,\n    learning_rate=1e-4,                 \n    weight_decay=0.01,\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n\n    fp16=True,\n    gradient_checkpointing=True,      \n\n    eval_strategy=\"steps\",\n    eval_steps=1000,                   \n    save_strategy=\"steps\",\n    save_steps=1000,                   \n    logging_steps=50,\n\n    max_steps=max_steps,\n\n    optim=\"paged_adamw_8bit\",\n    report_to=\"none\",\n    dataloader_num_workers=2,\n    torch_empty_cache_steps=500,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:17:00.165729Z","iopub.execute_input":"2025-10-06T03:17:00.166066Z","iopub.status.idle":"2025-10-06T03:17:00.202437Z","shell.execute_reply.started":"2025-10-06T03:17:00.166043Z","shell.execute_reply":"2025-10-06T03:17:00.201806Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# TRAINER AND TRAINING","metadata":{}},{"cell_type":"code","source":"from transformers import default_data_collator\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=default_data_collator,\n)\n\nprint(f\"Using {trainer.args.n_gpu} GPU(s)\")\nprint(f\"Effective batch size (tokens side aside): {per_device_bs * grad_accum * max(1, trainer.args.n_gpu)}\")\nprint(\"Starting QLoRA fine-tuning...\")\ntrain_result = trainer.train()\nprint(train_result)\n\neval_results = trainer.evaluate()\nimport math, torch as th\nprint(f\"Eval loss: {eval_results['eval_loss']:.4f}\")\nprint(f\"Perplexity: {th.exp(th.tensor(eval_results['eval_loss'])):.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T03:17:02.194334Z","iopub.execute_input":"2025-10-06T03:17:02.195100Z","iopub.status.idle":"2025-10-06T07:27:23.774315Z","shell.execute_reply.started":"2025-10-06T03:17:02.195075Z","shell.execute_reply":"2025-10-06T07:27:23.773427Z"}},"outputs":[{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPU(s)\nEffective batch size (tokens side aside): 64\nStarting QLoRA fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10000/10000 4:09:43, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>1.158100</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.133200</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.112000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>1.099600</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>1.098100</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>1.092500</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>1.091400</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>1.085000</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>1.088100</td>\n      <td>6.898125</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>1.083700</td>\n      <td>nan</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"TrainOutput(global_step=10000, training_loss=1.1157851959228515, metrics={'train_runtime': 14987.9868, 'train_samples_per_second': 42.701, 'train_steps_per_second': 0.667, 'total_flos': 8.375552619105485e+16, 'train_loss': 1.1157851959228515, 'epoch': 2.4515812699190977})\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='159' max='159' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [159/159 00:32]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Eval loss: nan\nPerplexity: nan\n","output_type":"stream"}],"execution_count":21}]}